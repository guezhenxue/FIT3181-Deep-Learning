{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guezhenxue/FIT3181-Deep-Learning/blob/main/Tutorial/%20FIT3181_5215_Tute1b_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsbuejjOgQUa"
      },
      "source": [
        "# <span style=\"color:#0b486b\">  FIT3181/5215: Deep Learning (2025)</span>\n",
        "***\n",
        "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
        "*Lecturer (Clayton):* **A/Prof Zongyuan Ge** | zongyuan.ge@monash.edu <br/>\n",
        "*Lecturer (Malaysia):*  **Dr Arghya Pal** | arghya.pal@monash.edu <br/>\n",
        "*Lecturer (Malaysia):*  **Dr Lim Chern Hong** | lim.chernhong@monash.edu <br/>  <br/>\n",
        "*Head Tutor 3181:*  **Ms Ruda Nie H** |  RudaNie.H@monash.edu <br/>\n",
        "*Head Tutor 5215:*  **Ms Leila Mahmoodi** |  leila.mahmoodi@monash.edu\n",
        "\n",
        "<br/> <br/>\n",
        "Faculty of Information Technology, Monash University, Australia\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfD8qvjPgZyh"
      },
      "source": [
        "# Tutorial 1b: Logistic Regression with PyTorch\n",
        "\n",
        "\n",
        "This tutorial aims to introduce the Logistic Regression which can be regarded as a feed-forward neural network with one layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibELGzvaikdK"
      },
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"A\")"
      ],
      "metadata": {
        "id": "QkzmMc4Y6cEO",
        "outputId": "c16458f2-a683-497a-98f8-57418f3f65b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KvWnbflf5da"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler      # for feature scaling\n",
        "from sklearn.model_selection import train_test_split  # for train/test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogwAZF7jD30"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edfcjVt7jmND"
      },
      "source": [
        "We first load the `breast cancer` dataset from `sklean` datasets and then split into 80% for training and 20% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzXzzS1cgqnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d910de8-55e6-4cca-ccb9-f0b077b546bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of samples: 569, number of features: 30\n"
          ]
        }
      ],
      "source": [
        "# Prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(f'number of samples: {n_samples}, number of features: {n_features}')\n",
        "\n",
        "# split data to 80% for training and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw7fZ63623i_"
      },
      "source": [
        "**<span style=\"color:red\">Exercise 1</span>:** Write the code to print out the first 10 feature vectors in `X_train` and `y_train`. Write the code to show the unique labels in `y_train`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-mi6wAT6Iav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b26a63-0bbc-4e4e-9853-133952e3290a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3618, -0.2652, -0.3172, -0.4671,  1.8038,  1.1817, -0.5169,  0.1065,\n",
            "         -0.3901,  1.3914,  0.1437, -0.1208,  0.1601, -0.1326, -0.5863, -0.1248,\n",
            "         -0.5787,  0.1091, -0.2819, -0.1889, -0.2571, -0.2403, -0.2442, -0.3669,\n",
            "          0.5449,  0.2481, -0.7109, -0.0797, -0.5280,  0.2506],\n",
            "        [-0.8633,  0.7156, -0.8565, -0.7967, -0.0586, -0.4285, -0.5170, -0.6814,\n",
            "          0.7948,  0.3882, -0.4545,  0.4009, -0.4357, -0.5216, -1.1631,  0.2724,\n",
            "          0.0675, -0.2392,  1.1130,  0.3502, -0.8894,  0.3847, -0.8880, -0.7897,\n",
            "         -1.0429, -0.4824, -0.5631, -0.7698,  0.4431, -0.2099],\n",
            "        [-0.4334,  0.3251, -0.4129, -0.5036,  0.2029,  0.3169,  0.2114,  0.2923,\n",
            "         -0.2941,  1.1295, -0.2249,  0.9890, -0.0743, -0.4596,  1.8909,  0.8176,\n",
            "          0.5919,  1.7726,  0.1356,  0.7924, -0.6160, -0.0636, -0.5528, -0.6284,\n",
            "         -0.1823, -0.1924, -0.2601, -0.0660, -1.1169,  0.0329],\n",
            "        [-0.4191,  1.0410, -0.3904, -0.4502,  1.1198,  0.4183,  0.2901,  0.5127,\n",
            "          0.3334,  0.4426,  0.0808, -0.1050,  0.0346, -0.0833,  0.2272,  0.0507,\n",
            "         -0.0820,  0.1819, -0.5122, -0.0746,  0.1686,  1.2218,  0.1297,  0.0124,\n",
            "          2.2445,  0.9297,  0.5803,  0.8613,  0.7972,  1.0085],\n",
            "        [-1.0237, -0.2420, -1.0508, -0.9044, -1.0732, -1.1425, -0.8710, -0.9779,\n",
            "          2.0571, -0.7337, -0.8157,  2.2826, -0.7971, -0.6329,  1.1806, -0.6738,\n",
            "         -0.3924, -0.1015, -0.4079,  0.2627, -1.1148, -0.4106, -1.1410, -0.9145,\n",
            "         -1.5058, -1.1813, -1.1047, -1.4059, -0.3344, -0.9030],\n",
            "        [ 0.6039,  0.0346,  0.7377,  0.4600,  0.4358,  1.6256,  1.6753,  1.1131,\n",
            "          1.2414,  0.4254, -0.3268, -0.4416,  0.1067, -0.1620,  0.6865,  1.6032,\n",
            "          1.5351,  1.6460,  0.9361,  0.9680,  0.2312, -0.4524,  0.4380,  0.1037,\n",
            "          0.2049,  1.1782,  1.4508,  0.9509,  0.6896,  0.3806],\n",
            "        [-0.8633, -0.6371, -0.8989, -0.7889, -1.0668, -1.2626, -0.9490, -0.9109,\n",
            "         -1.1209, -0.1899, -0.9640, -0.4742, -0.9529, -0.7134, -0.3884, -1.0614,\n",
            "         -0.7260, -0.8804, -0.0362, -0.5380, -0.9604, -0.8959, -0.9913, -0.8172,\n",
            "         -1.2881, -1.2013, -1.0771, -1.1294, -0.8572, -0.7174],\n",
            "        [-0.1584, -0.3535, -0.2427, -0.2396, -1.2115, -1.2209, -0.8705, -0.9450,\n",
            "         -0.8920, -1.0886, -0.5354, -0.1260, -0.5925, -0.4186, -1.0833, -1.1159,\n",
            "         -0.6863, -1.0616, -0.8826, -1.0116, -0.1632,  0.0376, -0.2561, -0.2525,\n",
            "         -1.2021, -1.1143, -0.7973, -0.9562, -0.9366, -1.1560],\n",
            "        [-0.8547,  0.4762, -0.8789, -0.7912,  0.3788, -0.8524, -1.0082, -0.9869,\n",
            "         -0.7370, -0.1956, -0.4306,  0.7702, -0.4488, -0.4947, -0.0560, -0.8866,\n",
            "         -0.8028, -0.8851,  0.8356, -0.6580, -0.8185,  0.4907, -0.8443, -0.7517,\n",
            "         -0.2038, -0.9001, -1.1235, -1.0896,  0.1238, -0.6961],\n",
            "        [-0.8833, -1.0229, -0.8565, -0.8096,  0.4857, -0.2557, -0.4454, -0.4010,\n",
            "         -0.1428,  0.8963, -0.8956, -0.7273, -0.7578, -0.6876,  0.1088, -0.3734,\n",
            "         -0.1501, -0.2613,  0.3952, -0.3335, -0.9103, -0.9586, -0.8294, -0.7836,\n",
            "          0.3814, -0.2172, -0.2215, -0.1115,  0.6615,  0.3507]])\n",
            "tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1])\n",
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "#Your answer here\n",
        "print(X_train[:10])\n",
        "print(y_train[:10])\n",
        "print(np.unique(y_train))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb0to6spkf30"
      },
      "source": [
        "We use `StandardScaler()` from `sklearn` to normalize the training/testing sets. We convert the training/testing numpy arrays to PyTorch arrays and then reshape them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo8WXCEBjQ2H"
      },
      "outputs": [],
      "source": [
        "# scale data\n",
        "sc = StandardScaler()\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# convert to tensors\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.int64))\n",
        "y_test = torch.from_numpy(y_test.astype(np.int64))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3FScpK3nrsW"
      },
      "source": [
        "## Training/Testing Procedure\n",
        "\n",
        "We now present the `fundamental workflow of PyTorch` including training a model based on the training set and testing the trained model on the testing set. This fundamental workflow is the same for various PyTorch models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9TmWoxulW4j"
      },
      "source": [
        "### Prepare Model\n",
        "\n",
        "First, we need to declare and define a model, which is a computational graph showing how to compute the model output from the input vector $x$. Specifically, given a data point $x$ (i.e., [1,30]) a batch $x$ (i.e., [64,30]), or even the entire training set $x$ (i.e., [569,30]), we compute\n",
        "- logits = xW + b\n",
        "- pred_probs = softmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VeNzXPcjR_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214d789a-ab87-4cec-cc18-092dc2c03469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device = cpu\n"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "# f = wx + b, softmax at the end\n",
        "class LogisticRegression(nn.Module):\n",
        "\n",
        "    def __init__(self, n_input_features):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(n_input_features, 2)\n",
        "        self.X_train, self.y_train = None, None\n",
        "        self.X_test, self.y_test = None, None\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear(x)\n",
        "        # pred_probs = torch.nn.Softmax(dim=-1)(logits) #for asking question only\n",
        "        return logits #return the logits\n",
        "\n",
        "    def train_model(self, X_train, y_train, learning_rate, num_epochs, loss_fn, optimizer):\n",
        "        self.X_train, self.y_train = X_train.to(device), y_train.to(device) #load the data to device (GPU or CPU)\n",
        "        for epoch in range(num_epochs):\n",
        "            # forward pass and loss\n",
        "            logits = self.forward(self.X_train)\n",
        "\n",
        "            loss = loss_fn(logits, self.y_train.squeeze().long())\n",
        "\n",
        "            # backward pass to compute the gradient\n",
        "            loss.backward()\n",
        "\n",
        "            # updates the model parameter based on the gradient\n",
        "            optimizer.step()\n",
        "\n",
        "            # zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if (epoch+1) % 10 == 0:\n",
        "                print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        # Ensure the model is in evaluation mode\n",
        "        self.eval()\n",
        "\n",
        "        # Disable gradient calculation\n",
        "        with torch.no_grad():\n",
        "            # Load the data to the device (GPU or CPU)\n",
        "            self.X_test, self.y_test = X_test.to(device), y_test.to(device)\n",
        "            # Get the model's predictions\n",
        "            logits = self.forward(self.X_test.type(torch.float32))\n",
        "            # Compute the predicted class\n",
        "            y_predicted = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Calculate the number of correct predictions\n",
        "            corrects = (y_predicted == self.y_test).sum().item()\n",
        "            print(f'correct = {corrects}')\n",
        "\n",
        "            # Get the total number of samples\n",
        "            totals = self.y_test.size(0)\n",
        "            print(f'totals = {totals}')\n",
        "\n",
        "            # Compute the accuracy\n",
        "            acc = corrects / totals\n",
        "            print(f'accuracy = {acc * 100:.2f}%')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'device = {device}')\n",
        "# model = LogisticRegression(n_features).to(device)  #load the model to the current device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvtTrPQ0DT7g"
      },
      "source": [
        "**<span style=\"color:red\">Exercise 2</span>:** Explain the forward function. What are the meanings and dimensions of `logits` and `pred_probs`?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a mini-batch $x$, `logits` represents the logits/discriminative scores/values of data points in the batch, while `pred_probs` represents the prediction probabilities of data points in the batch"
      ],
      "metadata": {
        "id": "CvHiTin0l1NY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "179EXh3XmDqe"
      },
      "source": [
        "### Prepare Loss and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rSEFHvn4NAq"
      },
      "source": [
        "We declare `loss_fn` as the cross entropy loss. To train our logistic regression, we invoke the SGD optimizer with the learning rate $0.01$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NvIujMFlyP3"
      },
      "outputs": [],
      "source": [
        "# Loss and optimizer\n",
        "learning_rate = 0.01\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape, y_train.squeeze().shape)"
      ],
      "metadata": {
        "id": "Ogg8TSQOL1E6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b678228-fd38-408b-bc8f-c301cb6d769f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([455]) torch.Size([455])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86QKl-TfmqQV"
      },
      "source": [
        "### Train Model By Feeding the Training Set All-in-Once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPRYtcCl8vcl"
      },
      "source": [
        "We train the model in $200$ epochs (i.e., going through the entire training set $100$ times). Here in each epoch, we input entire training set to the model to compute the cross-entropy loss over the training set and then use the optimizer to update the model parameters (i.e., W and b)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I_P8kWLl9Da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aed8117-ad58-4627-90c2-9f722323c82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 0.5796\n",
            "epoch: 20, loss = 0.4290\n",
            "epoch: 30, loss = 0.3549\n",
            "epoch: 40, loss = 0.3097\n",
            "epoch: 50, loss = 0.2784\n",
            "epoch: 60, loss = 0.2551\n",
            "epoch: 70, loss = 0.2368\n",
            "epoch: 80, loss = 0.2219\n",
            "epoch: 90, loss = 0.2094\n",
            "epoch: 100, loss = 0.1988\n",
            "epoch: 110, loss = 0.1895\n",
            "epoch: 120, loss = 0.1814\n",
            "epoch: 130, loss = 0.1741\n",
            "epoch: 140, loss = 0.1676\n",
            "epoch: 150, loss = 0.1618\n",
            "epoch: 160, loss = 0.1565\n",
            "epoch: 170, loss = 0.1517\n",
            "epoch: 180, loss = 0.1472\n",
            "epoch: 190, loss = 0.1432\n",
            "epoch: 200, loss = 0.1394\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # forward pass and loss\n",
        "    X_train, y_train = X_train.to(device), y_train.to(device) #load the data to device (GPU or CPU)\n",
        "    logits = model(X_train)\n",
        "\n",
        "    loss = loss_fn(logits, y_train.squeeze().long())\n",
        "\n",
        "    # backward pass to compute the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # updates the model parameter based on the gradient\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(n_features).to(device)  #load the model to the current device\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSlNX6Aj1mCi",
        "outputId": "07accab1-620a-4c6b-c2c7-83571d60c714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(\n",
              "  (linear): Linear(in_features=30, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ewm-p0dnIYw"
      },
      "source": [
        "### Evaluate Trained Model on Testing Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j8AlzmY92no"
      },
      "source": [
        "We compute the accuracy on the testing set (i.e., the testing accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Load the data to the device (GPU or CPU)\n",
        "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "    # Get the model's predictions\n",
        "    logits = model(X_test.type(torch.float32))\n",
        "    # Compute the predicted class\n",
        "    y_predicted = torch.argmax(logits, dim=1)\n",
        "\n",
        "    # Calculate the number of correct predictions\n",
        "    corrects = (y_predicted == y_test).sum().item()\n",
        "    print(f'correct = {corrects}')\n",
        "\n",
        "    # Get the total number of samples\n",
        "    totals = y_test.size(0)\n",
        "    print(f'totals = {totals}')\n",
        "\n",
        "    # Compute the accuracy\n",
        "    acc = corrects / totals\n",
        "    print(f'accuracy = {acc * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "QUO6rdh3AVVg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5bc76c-26b4-4abb-feef-fd6f19c6911c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct = 79\n",
            "totals = 114\n",
            "accuracy = 69.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xs8K0n1_-fY"
      },
      "source": [
        "**<span style=\"color:red\">Exercise 3</span>:** Explain the code above to compute the testing accuracy. What are `logits` and `y_predicted`?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`logits` is a 2D tensor of the shape $[114,2]$ in which each row is the logits of a data point in the testing set. `y_predicted` is a 1D tensor that contains the predicted labels of the data points in the testing set. You can print out the values of `logits` and `y_predicted` for more information."
      ],
      "metadata": {
        "id": "frNq41a-mcGh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvzd5_3YBJ7d"
      },
      "source": [
        "**<span style=\"color:red\">Exercise 4</span>:** Package the above code in a function, allowing you to try with different learning rates. Then, train the logistic regression models with different learning rates (i.e., 0.05, 0.04, 0.005, 0.001) and observe the loss tendency and testing accuracies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_logistic_regression(learning_rate, n_features, X_train, y_train, X_test, y_test, num_epochs):\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = LogisticRegression(n_features).to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(f\"\\nTraining with learning rate: {learning_rate}\")\n",
        "\n",
        "    # Train the model\n",
        "    model.train_model(X_train, y_train, learning_rate, num_epochs, loss_fn, optimizer)\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.evaluate(X_test, y_test)\n",
        "\n",
        "# Train and evaluate with different learning rates\n",
        "learning_rates_to_try = [0.05, 0.04, 0.005, 0.001]\n",
        "for lr in learning_rates_to_try:\n",
        "    train_and_evaluate_logistic_regression(lr, n_features, X_train, y_train, X_test, y_test, num_epochs)"
      ],
      "metadata": {
        "id": "rDq3Vpn9nvsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5e2df1-1247-4b4e-8a31-bb589b09190c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with learning rate: 0.05\n",
            "epoch: 10, loss = 0.2527\n",
            "epoch: 20, loss = 0.1800\n",
            "epoch: 30, loss = 0.1473\n",
            "epoch: 40, loss = 0.1282\n",
            "epoch: 50, loss = 0.1156\n",
            "epoch: 60, loss = 0.1066\n",
            "epoch: 70, loss = 0.0997\n",
            "epoch: 80, loss = 0.0944\n",
            "epoch: 90, loss = 0.0900\n",
            "epoch: 100, loss = 0.0863\n",
            "epoch: 110, loss = 0.0832\n",
            "epoch: 120, loss = 0.0805\n",
            "epoch: 130, loss = 0.0782\n",
            "epoch: 140, loss = 0.0761\n",
            "epoch: 150, loss = 0.0742\n",
            "epoch: 160, loss = 0.0725\n",
            "epoch: 170, loss = 0.0710\n",
            "epoch: 180, loss = 0.0696\n",
            "epoch: 190, loss = 0.0683\n",
            "epoch: 200, loss = 0.0671\n",
            "correct = 110\n",
            "totals = 114\n",
            "accuracy = 96.49%\n",
            "\n",
            "Training with learning rate: 0.04\n",
            "epoch: 10, loss = 0.2969\n",
            "epoch: 20, loss = 0.2077\n",
            "epoch: 30, loss = 0.1690\n",
            "epoch: 40, loss = 0.1465\n",
            "epoch: 50, loss = 0.1317\n",
            "epoch: 60, loss = 0.1210\n",
            "epoch: 70, loss = 0.1130\n",
            "epoch: 80, loss = 0.1066\n",
            "epoch: 90, loss = 0.1014\n",
            "epoch: 100, loss = 0.0971\n",
            "epoch: 110, loss = 0.0934\n",
            "epoch: 120, loss = 0.0902\n",
            "epoch: 130, loss = 0.0874\n",
            "epoch: 140, loss = 0.0849\n",
            "epoch: 150, loss = 0.0827\n",
            "epoch: 160, loss = 0.0807\n",
            "epoch: 170, loss = 0.0789\n",
            "epoch: 180, loss = 0.0773\n",
            "epoch: 190, loss = 0.0758\n",
            "epoch: 200, loss = 0.0743\n",
            "correct = 107\n",
            "totals = 114\n",
            "accuracy = 93.86%\n",
            "\n",
            "Training with learning rate: 0.005\n",
            "epoch: 10, loss = 0.6158\n",
            "epoch: 20, loss = 0.4858\n",
            "epoch: 30, loss = 0.4100\n",
            "epoch: 40, loss = 0.3609\n",
            "epoch: 50, loss = 0.3261\n",
            "epoch: 60, loss = 0.3000\n",
            "epoch: 70, loss = 0.2795\n",
            "epoch: 80, loss = 0.2629\n",
            "epoch: 90, loss = 0.2491\n",
            "epoch: 100, loss = 0.2374\n",
            "epoch: 110, loss = 0.2274\n",
            "epoch: 120, loss = 0.2186\n",
            "epoch: 130, loss = 0.2108\n",
            "epoch: 140, loss = 0.2039\n",
            "epoch: 150, loss = 0.1977\n",
            "epoch: 160, loss = 0.1920\n",
            "epoch: 170, loss = 0.1869\n",
            "epoch: 180, loss = 0.1822\n",
            "epoch: 190, loss = 0.1779\n",
            "epoch: 200, loss = 0.1739\n",
            "correct = 105\n",
            "totals = 114\n",
            "accuracy = 92.11%\n",
            "\n",
            "Training with learning rate: 0.001\n",
            "epoch: 10, loss = 0.5198\n",
            "epoch: 20, loss = 0.4988\n",
            "epoch: 30, loss = 0.4800\n",
            "epoch: 40, loss = 0.4629\n",
            "epoch: 50, loss = 0.4475\n",
            "epoch: 60, loss = 0.4334\n",
            "epoch: 70, loss = 0.4204\n",
            "epoch: 80, loss = 0.4086\n",
            "epoch: 90, loss = 0.3977\n",
            "epoch: 100, loss = 0.3876\n",
            "epoch: 110, loss = 0.3782\n",
            "epoch: 120, loss = 0.3695\n",
            "epoch: 130, loss = 0.3613\n",
            "epoch: 140, loss = 0.3537\n",
            "epoch: 150, loss = 0.3466\n",
            "epoch: 160, loss = 0.3399\n",
            "epoch: 170, loss = 0.3336\n",
            "epoch: 180, loss = 0.3276\n",
            "epoch: 190, loss = 0.3220\n",
            "epoch: 200, loss = 0.3166\n",
            "correct = 97\n",
            "totals = 114\n",
            "accuracy = 85.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1txQokl3Uvf"
      },
      "source": [
        "----\n",
        "\n",
        "**The end**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elDXDgHL3VWY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}